{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Cj-rL6xHlA2u"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMBIARLE EL NOMBRE EL SCRIPT A prediccion_intermedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp /home/eanegrin/buckets/b1/datasets/competencia_02_fe_v01.parquet /home/eanegrin/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8jGKjoN1lRho"
   },
   "outputs": [],
   "source": [
    "# base_path = '/content/drive/MyDrive/DMEyF/2024/'\n",
    "base_path = 'C:/Eugenio/Maestria/DMEyF/'\n",
    "# base_path = '/home/eanegrin/buckets/b1/'\n",
    "\n",
    "dataset_path = base_path + 'datasets/'\n",
    "modelos_path = base_path + 'modelos/'\n",
    "db_path = base_path + 'db/'\n",
    "dataset_file = 'competencia_02_fe_v01_undersampled_10_24M.parquet'\n",
    "\n",
    "ganancia_acierto = 273000\n",
    "costo_estimulo = 7000\n",
    "\n",
    "# agregue sus semillas\n",
    "semillas = [122219, 109279, 400391, 401537, 999961]\n",
    "\n",
    "# data = pd.read_parquet('/home/eanegrin/datasets/' + dataset_file)\n",
    "data = pd.read_parquet(dataset_path + dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391550, 679)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meses_train = [201906, 201907, 201908, 201909, 201910, 201911, 201912,\n",
    "               202001, 202002, 202003, 202004, 202005, 202006,\n",
    "               202007, 202008, 202009, 202010, 202011, 202012,\n",
    "               202101, 202102, 202103, 202104, 202105] # dejo afuera 202106 para test\n",
    "\n",
    "data = data[data['foto_mes'].isin(meses_train)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos pesos a las clases\n",
    "\n",
    "data['clase_peso'] = 1.0\n",
    "\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clase_binaria'] = 0\n",
    "data['clase_binaria'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria'], axis=1)\n",
    "y_train_binaria = data['clase_binaria'] # Junta a los 2 baja\n",
    "w_train = data['clase_peso']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar la calidad del modelo, crearemos nuestra propia función de evaluación que calcule la ganancia. La razón de incluir los pesos es precisamente para poder implementar esta función de evaluación de manera adecuada. Al combinar las clases *BAJA+1* y *BAJA+2* en una sola, necesitamos una forma de diferenciarlas, y es aquí donde entra en juego el *weight*. Este parámetro nos permitirá distinguir entre ambas clases al momento de evaluarlas dentro del algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_gan_eval(y_pred, data):\n",
    "    weight = data.get_weight()\n",
    "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
    "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
    "    ganancia = np.cumsum(ganancia)\n",
    "\n",
    "    return 'gan_eval', np.max(ganancia) , True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el study de optuna que optimizamos en el script anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYMEnNFbkSoQ",
    "outputId": "15d768d4-9e25-4063-984b-d79c90ea91bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 14:34:48,684] Using an existing study with name 'competencia2_lgbm_v05' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
    "study_name = \"competencia2_lgbm_v05\" # UPDATE\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados = study.trials_dataframe()\n",
    "resultados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos 5 columnas con las predicciones para cada semilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos cross-validation para que las nuevas columnas no esten distorsionadas por el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n"
     ]
    }
   ],
   "source": [
    "# Pre training to add predictions of the 5 seeds\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "version = 'v007' # UPDATE\n",
    "\n",
    "# Initialize an empty array to hold the out-of-fold predictions for each model (seed)\n",
    "predictions = np.zeros((X_train.shape[0], len(semillas)))\n",
    "\n",
    "# Set up cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for idx, semilla in enumerate(semillas):\n",
    "    \n",
    "    best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "    print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': study.best_trial.params['num_leaves'],\n",
    "        'learning_rate': study.best_trial.params['learning_rate'],\n",
    "        'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
    "        'feature_fraction': study.best_trial.params['feature_fraction'],\n",
    "        'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
    "        'seed': semilla,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    # Loop through each fold in the StratifiedKFold\n",
    "    for fold_idx, (train_index, val_index) in enumerate(kf.split(X_train, y_train_binaria)):\n",
    "        \n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_binaria.iloc[train_index], y_train_binaria.iloc[val_index]\n",
    "    \n",
    "        w_train_fold = w_train.iloc[train_index]\n",
    "        w_val_fold = w_train.iloc[val_index]\n",
    "\n",
    "        # Create LightGBM Dataset\n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold, weight=w_train_fold)\n",
    "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, weight=w_val_fold, reference=train_data)\n",
    "        \n",
    "        # Train the model\n",
    "        model = lgb.train(params,\n",
    "                          train_data,\n",
    "                          num_boost_round=best_iter)\n",
    "        \n",
    "        # Predict on the validation fold and store it in the oof_preds matrix\n",
    "        predictions[val_index, idx] = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "\n",
    "# At the end of the loop, oof_preds contains the predictions for each fold and seed\n",
    "# You can now add these predictions as columns in your original dataframe\n",
    "\n",
    "X_train_with_preds = X_train.copy()\n",
    "for i, semilla in enumerate(semillas):\n",
    "    X_train_with_preds[f'pred_s{semilla}'] = predictions[:, i]\n",
    "\n",
    "# Now, X_train_with_preds has the original data plus the predictions of each fold for each seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391550, 682)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_with_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar las columnas en test no vamos a poder hacer cross validation por lo que entrenamos los modelos intermedios para levantarlos en el script de test y generar las 5 columnas de predicciones, luego sobre ese dataset es donde aplicamos el predict de los modelos finales que se estiman mas abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n"
     ]
    }
   ],
   "source": [
    "version = 'v007' # UPDATE\n",
    "\n",
    "for semilla in semillas:\n",
    "    \n",
    "    best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "    print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': study.best_trial.params['num_leaves'],\n",
    "        'learning_rate': study.best_trial.params['learning_rate'],\n",
    "        'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
    "        'feature_fraction': study.best_trial.params['feature_fraction'],\n",
    "        'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
    "        'seed': semilla,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, # Entrenamos ahora si con las 5 nuevas columnas agregadas\n",
    "                            label=y_train_binaria,\n",
    "                            weight=w_train)\n",
    "\n",
    "    model = lgb.train(params,\n",
    "                    train_data,\n",
    "                    num_boost_round=best_iter)\n",
    "    \n",
    "    model.save_model(modelos_path + f'{version}/lgb_competencia2_{version}_s{semilla}_intermediate.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO MAS QUE SE PODRIA PROBAR ES AGREGAR REZAGOS DE ESTAS PROBABILIDADES, CON LA LOGICA DE QUE SI UN CLIENTE TIENE ALTAS PROBABILIDADES EN LOS ULTIMOS MESES EVIDENTEMENTE ESTA AL BORDE\n",
    "PARA HACER ESO EL PASO ANTERIOR DEBERIA HACERLO CON TODOS LOS DATOS, NO CON EL DATASET UNDERSAMPLEADO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos y guardamos los modelos que vamos a evaluar en Test\n",
    "\n",
    "Tenemos entonces los siguientes modelos:\n",
    "\n",
    "- los usados en cross validation para generar las columnas de prediccion en el set de entrenamiento\n",
    "\n",
    "- los entrenados con todo el test de entrenamiento (sin las columnas de prediccion), a estos los llamamos intermediate y son los que vamos a usar para generar las columnas de\n",
    "  prediccion en test ya que ahi no podemos usar la cross validation\n",
    "\n",
    "- modelo \"final\" del conjunto de entrenamiento, los que entrenamos en el set de entrenamiento (con las columnas de prediccion), estos son los que vamos a evaluar en test luego de agregar  las columnas de prediccion\n",
    "\n",
    "- modelo final, volvemos a generar las columnas de prediccion con el dataset de entrenamiento entero (usando cv como en el paso 1) y luego entrenamos los modelos finales sobre el conjunto con las predicciones agregadas.\n",
    "\n",
    "- tambien generamos los modelos intermediate_final que son los que vamos a usar para generar las columnas de prediccion en el dataset a predecir, y luego para predecir vamos a aplicar el modelo final sobre este nuevo conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n",
      "Mejor cantidad de árboles para el mejor model 996\n"
     ]
    }
   ],
   "source": [
    "version = 'v007' # UPDATE\n",
    "\n",
    "for semilla in semillas:\n",
    "    \n",
    "    best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "    print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': study.best_trial.params['num_leaves'],\n",
    "        'learning_rate': study.best_trial.params['learning_rate'],\n",
    "        'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
    "        'feature_fraction': study.best_trial.params['feature_fraction'],\n",
    "        'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
    "        'seed': semilla,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train_with_preds, # Entrenamos ahora si con las 5 nuevas columnas agregadas\n",
    "                            label=y_train_binaria,\n",
    "                            weight=w_train)\n",
    "\n",
    "    model = lgb.train(params,\n",
    "                    train_data,\n",
    "                    num_boost_round=best_iter)\n",
    "    \n",
    "    model.save_model(modelos_path + f'{version}/lgb_competencia2_{version}_s{semilla}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
